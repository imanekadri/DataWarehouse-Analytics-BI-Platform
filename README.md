# Plateforme dâ€™Analyse de DonnÃ©es avec PostgreSQL, Apache Airflow et Metabase

## Description

Ce projet vise Ã  concevoir et dÃ©ployer une plateforme complÃ¨te dâ€™analyse de donnÃ©es intÃ©grant toutes les Ã©tapes du cycle dÃ©cisionnel : collecte, transformation, stockage, analyse et visualisation des donnÃ©es.  
La solution repose sur des technologies open source et est entiÃ¨rement conteneurisÃ©e avec Docker pour assurer la portabilitÃ© et la reproductibilitÃ©.

La plateforme combine :  
- **PostgreSQL** : Data Warehouse relationnel pour centraliser et structurer les donnÃ©es.  
- **Apache Airflow** : Orchestration et automatisation des pipelines ETL.  
- **Metabase** : Visualisation et analyse dÃ©cisionnelle des donnÃ©es via des dashboards interactifs.

---

## Objectifs du projet

- Centraliser les donnÃ©es provenant de diffÃ©rentes sources dans un Data Warehouse.  
- Automatiser les processus ETL (Extract, Transform, Load) avec Airflow.  
- Permettre lâ€™analyse et la visualisation des donnÃ©es en quasi-temps rÃ©el avec Metabase.  
- CrÃ©er des tableaux de bord interactifs et des rapports exploitables pour la prise de dÃ©cision.

---

## Architecture du projet
                     Sources de donnÃ©es --> DAGs Airflow --> Data Warehouse (PostgreSQL) --> Dashboards Metabase

- **Sources de donnÃ©es** : fichiers CSV, bases de donnÃ©es externes ou autres flux de donnÃ©es.  
- **DAGs Airflow** : pipelines ETL planifiÃ©s pour extraire, transformer et charger les donnÃ©es.  
- **Data Warehouse PostgreSQL** : stockage structurÃ© des donnÃ©es avec schÃ©ma en Ã©toile (Star Schema).  
- **Metabase** : crÃ©ation de questions, graphiques et dashboards interactifs.

---

## Guide dâ€™Installation

### PrÃ©requis

- Avant de commencer, assurez-vous dâ€™avoir installÃ© sur votre machine :
- Docker (version rÃ©cente)
- Docker Compose
- Minimum recommandÃ© : 8 Go de RAM

### Cloner le repository
```bash
git clone https://github.com/imanekadri/DataWarehouse-Analytics-BI-Platform.git
cd DataWarehouse-Analytics-BI-Platform
```
### Configuration des conteneurs
Les services sont configurÃ©s via le fichier docker-compose.yml :

- PostgreSQL : base de donnÃ©es pour le Data Warehouse
- Apache Airflow : orchestration des pipelines ETL
- Metabase : visualisation et tableaux de bord
- Vous pouvez modifier les paramÃ¨tres (ports, mots de passe, volumes) directement dans le fichier docker-compose.yml si nÃ©cessaire.

### Lancer la plateforme

Pour dÃ©marrer tous les services, exÃ©cutez :

docker-compose up -d


Cette commande va :

- CrÃ©er les conteneurs
- DÃ©marrer PostgreSQL, Airflow et Metabase
- Monter les volumes pour persistance des donnÃ©es
- VÃ©rifier le fonctionnement

  
1. PostgreSQL

     - Host : localhost ou adresse du conteneur

     - Port : 5432

     - Database : DataWarehouse

     - Username / Password : dÃ©finis dans docker-compose

2. Apache Airflow

     - AccÃ©der Ã  lâ€™interface : http://localhost:8080

     - Les DAGs principaux : data_warehouse_daily, data_warehouse_full_etl

     - Vous pouvez exÃ©cuter les DAGs manuellement ou attendre leur planification automatique.

3. Metabase

     - AccÃ©der Ã  lâ€™interface : http://localhost:3000

     - Connecter la base PostgreSQL pour crÃ©er vos dashboards et questions.

     - ExÃ©cution des DAGs ETL

     - Aller dans lâ€™interface Airflow

     - Activer le DAG souhaitÃ©

     - Cliquer sur "Trigger DAG" pour exÃ©cution immÃ©diate

     - VÃ©rifier que les donnÃ©es sont chargÃ©es dans le Data Warehouse

### Ressources utiles

- Documentation Docker : https://docs.docker.com/

- Documentation Airflow : https://airflow.apache.org/docs/

- Documentation Metabase : https://www.metabase.com/docs/latest/

ðŸ’¡ Astuce : Pour arrÃªter et supprimer les conteneurs :
```bash
docker-compose down
```
 

## Structure du projet


/project
â”‚
â”œâ”€â”€ docker-compose.yml

â”œâ”€â”€ airflow/

â”‚   â”œâ”€â”€ dags/

â”‚       â””â”€â”€ data_warehouse_daily.py
        
â”‚       â””â”€â”€ data_warehouse_full_etl.py

â”‚   â””â”€â”€ plugins/

â”œâ”€â”€ sql-scripts/

â”‚             â””â”€â”€ create-airflow-db.sql

â”‚             â””â”€â”€ initial_database.sql
              
â”‚             â””â”€â”€ sample_data.sql
    
â””â”€â”€ README.md


### Ã‰tapes

1. Cloner le repository :
```bash
git clone https://votre-repository.git
cd nom-du-repo
docker-compose up -d
